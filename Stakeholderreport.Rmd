---
title: "M2-Stakeholderreport-Amazon"
date: "10/24/2019"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Introduction

This stakeholderreport is made in html and the code is hidden for you, just as the report. 

In this project we will work with a dataset of 5.000 consumer reviews for a few Amazon electronic products like f. ex. Kindle. Data is collected between September 2017 and October 2018.

```{r, include=FALSE}
### Knitr options
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     fig.align="center"
                     )

options(warn=-1) # Hides all warnings, as the knitr options only work on local R-Markdown mode. 

Sys.setenv(LANG = "en")
```

```{r, include=FALSE}
# Packages

if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
pacman::p_load(knitr, # For knitr to html
               rmarkdown, # For formatting the document
               tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               data.table, # for reading in data ect. 
               magrittr,# For advanced piping (%>% et al.)
               igraph, # For network analysis
               tidygraph, # For tidy-style graph manipulation
               ggraph, # For ggplot2 style graph plotting
               Matrix, # For some matrix functionality
               ggforce, # Awesome plotting
               kableExtra, # Formatting for tables
               car, # recode functions 
               tidytext, # Structure text within tidyverse
               topicmodels, # For topic modelling
               tm, # text mining library
               quanteda, # for LSA (latent semantic analysis)
               uwot, # for UMAP
               dbscan, # for density based clustering
               SnowballC,
               textdata,
               wordcloud, 
               textstem, # for textstemming 
               tidyr,
               widyr,
               reshape2,
               quanteda,
               uwot,
               dbscan,
               plotly,
               rsample,
               glmnet,
               doMC,
               broom,
               yardstick,
               lda, # For LDA-analysis
               topicmodels # LDA models
               )

# I set a seed for reproduciability
set.seed(123) # Have to be set every time a rng proces is being made. 
```

For this report we will only use a subset of the data we downloaded from kaggle. We select the following variables: 

- id: An id number given to each review created by us corrensponding to the row number of the raw data. 

- name: The full name of the product

- reviews.rating: The rating of the product on a scale from 1-5. 

- reviews.title: The title of the review, given by the customer. 

- reviews.text: The review text written by the customer. 

```{r}
data_raw <- read_csv("Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv") %>% 
  select(name, reviews.rating, reviews.text, reviews.title) %>% 
  mutate(id = row_number())

```

As the data is very raw and messy we now do some cleaning. We remove everything that is not normal letters. We also don't want to analyze the exact word strings in the reviews, as this would include several possible forms of the words used. F. ex. think and thought. Instead we want to merge all possible forms of a word into it's root word. This we do with lemmatization. We here want to primarily work with tidy text, where there is one token per row. A token here is a single word. 

```{r}
tokens_clean <- data_raw %>% 
  unnest_tokens(word, reviews.text, to_lower = TRUE) %>% 
  mutate(word = word %>% str_remove_all("[^a-zA-Z]")) %>%
  filter(str_length(word) > 0) %>% 
  mutate(word = lemmatize_words(word))

reviewtext_lemma <- tokens_clean %>% 
  group_by(id) %>% 
  summarize(reviews.text = str_c(word, collapse = " ")) %>% 
  ungroup() %>% 
  select(reviews.text) %>% 
  as_vector()

data_clean <- data_raw %>%
  mutate(reviews.text = reviewtext_lemma)

```

We now have 153.994 tokens, in their each seperate rows in the tokens dataset. By doing lemmatization the number of unique tokens are reduced from around 6000 to around 4600 words, which should prove quite beneficial. 

# Network analysis

In this assignment we want to use network analysis to gain new insights into how the reviews are structured. Here we extract bigrams from each review text, clean and prepare them to then create networks. Where we before considered tokens as individual words, we can create them as n-grams that are a consecutive sequence of words. Bigrams are n-grams with a length of 2 consecutive words. This can be used to gain context and connection between words. 

```{r}
bigrams <- data_clean %>%
  unnest_tokens(bigram, reviews.text, token = "ngrams", n = 2) # n is the number of words to consider in each n-gram. 

bigrams$bigram[1:2]
```

Remember that each bigram overlap, as can be seen from above, so that the first token is "the display" and the second is "display is". We also remove stopwords such as "i", "am", "the" and ect. from the bigrams, as these are not meaninfull for the analysis. These are taken from a dictionary and should remove most stopwords, but are not exhaustive. 

The interesting thing is now to visualize the relationship between all words. Before doing this we will need to create the graph from a data frame of the bigrams. Here nodes are the words, and the edges correspond to the connection between the two words in the bigram. The first word in the bigram is the column 'from', and the second word is 'to', and it's therefore a directed network. The edges are given a weight corresponding to how many times it occures in the total amount of reviews called 'n'. The weight is plotted as the alpha value, so more frequent bigrams have a darker colour, and vice versa. Only bigrams that occure more than 15 times are plotted in this network, as it otherwise would get to messy. 

```{r}
bigrams_separated <- bigrams %>% 
  separate(bigram,c("word1","word2"),sep = " ")

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

#New bigram counts
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

set.seed(123)
bigram_graph <- bigram_counts %>% 
  filter(n > 15) %>%  #The occurence of the bigram is more than 15. 
  graph_from_data_frame()

a<- grid::arrow(type = "closed",length = unit(.15,"inches"))

ggraph(bigram_graph, layout = "fr") + 
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.05,'inches'))+
  geom_node_point(color = "pink", size = 3) + 
  geom_node_text(aes(label = name),vjust=1,hjust=1) + 
  theme_void()
```

The plot above, give us some insights about the connection of words in the reviews. If we where to chose a random word in the graph, the most likely word to come afterwards would be the outgoing connection with the darkest colour. This way we can kinda predict what words that come next. Remember that the words have been lemmatized, so it shows the root word so the sentence created would not be grammatical correct, but would still carry the meaning as a whole. 

We see many small connections such as customer -> service, sound -> quality, black -> friday and ect. Then we also have a bigger cluster where love is one of the key words. Many words such as kid, daughter, son, wife ect. point in the direction of love, and then outgoing edges from love is play, watch, alexa. Creating sentences such as "wife love alexa" or "kid love play". So first we have the person, then the sentiment word love, and then the action they do or what they love. We see that amazon is a central word with many outgoing connections, as many things are called "amazon prime", "amazon account" ect. Other key nodes are the product names such as "fire", "kindle", "hue". 












