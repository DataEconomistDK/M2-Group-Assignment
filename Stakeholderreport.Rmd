---
title: "M2-Stakeholderreport-Amazon"
date: "10/24/2019"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Introduction

This stakeholderreport is made in html and the code is hidden for you, just as the report. 

In this project we will work with a dataset of 5.000 consumer reviews for a few Amazon electronic products like f. ex. Kindle. Data is collected between September 2017 and October 2018.

```{r, include=FALSE}
### Knitr options
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     fig.align="center"
                     )

options(warn=-1) # Hides all warnings, as the knitr options only work on local R-Markdown mode. 

Sys.setenv(LANG = "en")
```

```{r, include=FALSE}
# Packages

if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
pacman::p_load(knitr, # For knitr to html
               rmarkdown, # For formatting the document
               tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               data.table, # for reading in data ect. 
               magrittr,# For advanced piping (%>% et al.)
               igraph, # For network analysis
               tidygraph, # For tidy-style graph manipulation
               ggraph, # For ggplot2 style graph plotting
               Matrix, # For some matrix functionality
               ggforce, # Awesome plotting
               kableExtra, # Formatting for tables
               car, # recode functions 
               tidytext, # Structure text within tidyverse
               topicmodels, # For topic modelling
               tm, # text mining library
               quanteda, # for LSA (latent semantic analysis)
               uwot, # for UMAP
               dbscan, # for density based clustering
               SnowballC,
               textdata,
               wordcloud, 
               textstem, # for textstemming 
               tidyr,
               widyr,
               reshape2,
               quanteda,
               uwot,
               dbscan,
               plotly,
               rsample,
               glmnet,
               doMC,
               broom,
               yardstick,
               lda, # For LDA-analysis
               topicmodels # LDA models
               )

# I set a seed for reproduciability
set.seed(123) # Have to be set every time a rng proces is being made. 
```

For this report we will only use a subset of the data we downloaded from kaggle. We select the following variables: 

- id: An id number given to each review created by us corrensponding to the row number of the raw data. 

- name: The full name of the product

- reviews.rating: The rating of the product on a scale from 1-5. 

- reviews.title: The title of the review, given by the customer. 

- reviews.text: The review text written by the customer. 

```{r}
data_raw <- read_csv("Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv") %>% 
  select(name, reviews.rating, reviews.text, reviews.title) %>% 
  mutate(id = row_number())

```

As the data is very raw and messy we now do some cleaning. We remove everything that is not normal letters. We also don't want to analyze the exact word strings in the reviews, as this would include several possible forms of the words used. F. ex. think and thought. Instead we want to merge all possible forms of a word into it's root word. This we do with lemmatization. We here want to primarily work with tidy text, where there is one token per row. A token here is a single word. 

```{r}
tokens_clean <- data_raw %>% 
  unnest_tokens(word, reviews.text, to_lower = TRUE) %>% 
  mutate(word = word %>% str_remove_all("[^a-zA-Z]")) %>%
  filter(str_length(word) > 0) %>% 
  mutate(word = lemmatize_words(word))

reviewtext_lemma <- tokens_clean %>% 
  group_by(id) %>% 
  summarize(reviews.text = str_c(word, collapse = " ")) %>% 
  ungroup() %>% 
  select(reviews.text) %>% 
  as_vector()

data_clean <- data_raw %>%
  mutate(reviews.text = reviewtext_lemma)

```

We now have 153.994 tokens, in their each seperate rows in the tokens dataset. By doing lemmatization the number of unique tokens are reduced from around 6000 to around 4600 words, which should prove quite beneficial. 

# Network analysis

In this assignment we want to use network analysis to gain new insights into how the reviews are structured. Here we extract bigrams from each review text, clean and prepare them to then create networks. Where we before considered tokens as individual words, we can create them as n-grams that are a consecutive sequence of words. Bigrams are n-grams with a length of 2 consecutive words. This can be used to gain context and connection between words. 

```{r}
bigrams <- data_clean %>%
  unnest_tokens(bigram, reviews.text, token = "ngrams", n = 2) # n is the number of words to consider in each n-gram. 

bigrams$bigram[1:2]
```

Remember that each bigram overlap, as can be seen from above, so that the first token is "the display" and the second is "display is". We also remove stopwords such as "i", "am", "the" and ect. from the bigrams, as these are not meaninfull for the analysis. These are taken from a dictionary and should remove most stopwords, but are not exhaustive. 

The interesting thing is now to visualize the relationship between all words. Before doing this we will need to create the graph from a data frame of the bigrams. Here nodes are the words, and the edges correspond to the connection between the two words in the bigram. The first word in the bigram is the column 'from', and the second word is 'to', and it's therefore a directed network. The edges are given a weight corresponding to how many times it occures in the total amount of reviews called 'n'. The weight is plotted as the alpha value, so more frequent bigrams have a darker colour, and vice versa. Only bigrams that occure more than 15 times are plotted in this network, as it otherwise would get to messy. 

```{r}
bigrams_separated <- bigrams %>% 
  separate(bigram,c("word1","word2"),sep = " ")

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)

#New bigram counts
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

set.seed(123)
bigram_graph <- bigram_counts %>% 
  filter(n > 15) %>%  #The occurence of the bigram is more than 15. 
  graph_from_data_frame()

a<- grid::arrow(type = "closed",length = unit(.15,"inches"))

ggraph(bigram_graph, layout = "fr") + 
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.05,'inches'))+
  geom_node_point(color = "pink", size = 3) + 
  geom_node_text(aes(label = name),vjust=1,hjust=1) + 
  theme_void()
```

The plot above, give us some insights about the connection of words in the reviews. If we where to chose a random word in the graph, the most likely word to come afterwards would be the outgoing connection with the darkest colour. This way we can kinda predict what words that come next. Remember that the words have been lemmatized, so it shows the root word so the sentence created would not be grammatical correct, but would still carry the meaning as a whole. 

We see many small connections such as customer -> service, sound -> quality, black -> friday and ect. Then we also have a bigger cluster where love is one of the key words. Many words such as kid, daughter, son, wife ect. point in the direction of love, and then outgoing edges from love is play, watch, alexa. Creating sentences such as "wife love alexa" or "kid love play". So first we have the person, then the sentiment word love, and then the action they do or what they love. We see that amazon is a central word with many outgoing connections, as many things are called "amazon prime", "amazon account" ect. Other key nodes are the product names such as "fire", "kindle", "hue". 

# NLP

In this section we will analyze the data using Natural Language Processing. Here we will gain insight in the dataset by extracting and identifing patterns. We will start by doing some data preprocessing before we start our analysis. Then we will do a sentiment analysis, where we analysis the words and whether they're positive or negative. Next we will do a LSA analysis, where we will apply dimentionality reductions and cluster the data to look for latent patterns for the words and documents. And then we will do a LDA analysis to try to seperate the data into topics to look for patterns for the words.

```{r}
tokens_nlp <- tokens_clean %>% anti_join(stop_words)
own_stopwords <- tibble(word= c("im", "ive", "dont", "doesnt", "didnt"), 
                        lexicon = "OWN")
tokens_nlp <- tokens_clean %>% 
  anti_join(stop_words %>% bind_rows(own_stopwords), by = "word")
topwords <- tokens_nlp %>%
  count(word, sort=TRUE)
```

Before doing the sentiment analysis, we will quickly look a the distribution of the review ratings.

```{r}
summary(tokens_nlp$reviews.rating)
```

Here, we can see that there is a overepresentation of positive reviews, where the mean is at 4.533 and the median at 5.00. This will contribute to how we do the rest of the sentiment analysis. There is 1134 one-star review rating, 584 two-star review rating, 3028 three-star review rating, 12070 four-star review rating and 30383 five-star review rating. 

## Sentiment analysis

Sentiment analysis refers to a use of text analysis to extract and identify subjective information, where it analyzises whether the words are positive or negative. In this section, we will be doing two sentiment analysis, first by identifying positive and negative words using the bing lexicon and after this using the afinn lexicon. 

### Bing

The Bing lexicon categorizes words in a binary fashion as positive or negative with no weighting. We are now plotting a word count, grouped by sentiment, showing the 10 most frequent negative and positive words. 

```{r}
sentiment_bing <- tokens_nlp %>% inner_join(get_sentiments("bing"))

sentiment_analysis <- sentiment_bing %>% 
  filter(sentiment %in% c("positive", "negative"))

word_counts <- sentiment_analysis %>%
count(word, sentiment) %>%
group_by(sentiment) %>%
top_n(10, n) %>%
ungroup() %>%
mutate(
word2 = fct_reorder(word, n))

ggplot(word_counts, aes(x = word2, y = n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ sentiment, scales ="free") +
coord_flip() +
labs(title ="Sentiment Word Counts",x ="Words")
```

Here we can see the positive words are much more frequent than the negative words. For positive the words "love" and "easy" is way more frequent than all other words. 

And now we will count all positive and negative words for each number of stars. 

```{r}
tokens_nlp %>% 
  inner_join(get_sentiments("bing")) %>%
  count(reviews.rating, sentiment)
```

From the above table we can see that all categories of reviews both include positive and negative words. Even in 1 star rating reviews, almost a third of all sentiments words are positive. This might indicate that people use negative expressions such as "this is not very good", and then the only sentiment word is good which classifies as positive, but in reality the sentiment should be negative. This could be fixed with further analysis to remove the effect of negatives, but is not within the current time scope of this project. We also see that 2 star reviews have almost equal negative and positive sentiment words, which results in a almost neutral sentiment. 

If we sum the above sentiment words, we can see there is a total of 9.352 sentiment words in our data out of a total of 52.244 words, which means that around 20% of all words left is sentiment words. This means that most words are still some form of stopword, neutral sentiment word or a product word. In the bing lexicon, there's around 6000 unique sentiment words and in our data there is 345 unique sentiments words from the bing lexicon. 

Now we will find the overall sentiment score for every review rating, taking the positive sentiments and subtracting the negative. Then we take the mean and plot it. 

```{r, include=FALSE}
tokens_nlp_bing <- tokens_nlp %>%
  inner_join(get_sentiments("bing")) %>%
  count(reviews.rating, sentiment) %>%
  spread(sentiment, n) %>%
  mutate(overall_sentiment = positive - negative)

n1 <- tokens_nlp %>% filter(reviews.rating == 1)
s1 <- tokens_nlp_bing$overall_sentiment[1] / count(n1)
n2 <- tokens_nlp %>% filter(reviews.rating == 2)
s2 <- tokens_nlp_bing$overall_sentiment[2] / count(n2)
n3 <- tokens_nlp %>% filter(reviews.rating == 3)
s3 <- tokens_nlp_bing$overall_sentiment[3] / count(n3)
n4 <- tokens_nlp %>% filter(reviews.rating == 4)
s4 <- tokens_nlp_bing$overall_sentiment[4] / count(n4)
n5 <- tokens_nlp %>% filter(reviews.rating == 5)
s5 <- tokens_nlp_bing$overall_sentiment[5] / count(n5)

x <- c(s1,s2,s3,s4,s5)

ggplot(tokens_nlp_bing, aes(x = reviews.rating, y = x, fill = as.factor(reviews.rating))) + geom_col(show.legend = FALSE) + coord_flip() +
labs(title = "Overall Sentiment by Review rating",
     subtitle = "Reviews",
     x = "Review rating",
     y = "Overall Sentiment")
```

The scores are generally very low, because there wasn't a whole lot of words from the lexicon. But here we can see that the 1 score rating is much lower and almost have the same negative size as the five-star rating, which is actually an interesting insight.
















